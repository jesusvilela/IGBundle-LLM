base_model_id: "Qwen/Qwen2.5-7B"

ig_adapter:
  hidden_size: 3584
  num_components: 4
  num_categories: 16
  bottleneck_dim: 256
  latent_dim: 128
  alpha: 1.0
  beta: 1.0
  eta_f: 0.1
  eta_b: 0.01
  eta_w: 0.01
  adapter_scale: 0.1
  dropout: 0.05
  geometry: "euclidean"

lora:
  r: 8
  lora_alpha: 32
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  lora_dropout: 0.05

training:
  output_dir: "./output/igbundle_qwen7b_euclidean"
  num_train_epochs: 1
  max_steps: 25 # Very short run for rapid ablation
  per_device_train_batch_size: 1
  optim: paged_adamw_8bit
  gradient_accumulation_steps: 1
  learning_rate: 2e-4
  max_grad_norm: 0.3
  logging_steps: 1
  save_steps: 25 # Save at end
  bf16: true
  gradient_checkpointing: true

loss:
  lambda_glue: 0.01
  num_patches: 8
  tau: 1.0
