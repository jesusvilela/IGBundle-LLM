base_model_id: "Qwen/Qwen2.5-7B"

ig_adapter:
  hidden_size: 3584 # Qwen2.5-7B hidden size
  num_components: 4
  num_categories: 16
  bottleneck_dim: 256 # Bottleneck for parameter reduction
  latent_dim: 128 # Smaller dim for efficiency
  alpha: 1.0 # Base KL weight (affinity)
  beta: 1.0 # Fiber KL weight (affinity)
  eta_f: 0.1 # Fiber LR
  eta_b: 0.01 # Base LR
  eta_w: 0.01 # Weight LR
  adapter_scale: 0.1
  dropout: 0.05

lora:
  r: 8
  lora_alpha: 32
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  lora_dropout: 0.05

training:
  output_dir: "./output/igbundle_qwen7b"
  num_train_epochs: 1
  max_steps: 60 # Extended for full training check
  per_device_train_batch_size: 1 # Crucial for 8GB VRAM
  optim: paged_adamw_8bit
  gradient_accumulation_steps: 16
  learning_rate: 2e-5
  max_grad_norm: 0.3
  logging_steps: 1
  save_steps: 10
  bf16: true

loss:
  lambda_glue: 0.01
  num_patches: 8
  tau: 1.0
